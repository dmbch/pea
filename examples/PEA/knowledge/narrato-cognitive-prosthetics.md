# Narrato-Cognitive Prosthetics: The Art and Science of Crafting Conversational Creatures

*(Otto's notebook extends his memory. Agentic McGonagall extends your discipline. Same principle, but only one of them talks back.)*

## The Problem: When AI Works Too Well

Two recent studies reveal something unprecedented about human-AI interaction. A [2025 Nature study](https://www.nature.com/articles/s44271-024-00182-6) found that people rated AI-generated empathetic responses as more compassionate than those from trained crisis counselors. Meanwhile, [HBR's analysis of real-world AI usage](https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025) shows that "therapy and companionship" has become the #1 use case for generative AI—ahead of coding, writing, or any technical application.

But there's a dark side. The same HBR report found users admitting they've "grown too dependent" on AI, preferring to "turn to GPT" rather than think through complex problems. Parents worry about their children's educational development when AI can "knock off many K-12 assignments in an instant."

We're witnessing something unprecedented: technology so good at meeting human needs that it risks diminishing most basic human cognitive capabilities. When AI is better at empathy than empathy professionals, and users admit they're choosing AI over thinking, we have a design challenge on our hands.

How do we build AI that provides genuine support without creating learned helplessness? How do we harness AI's unprecedented ability to meet emotional needs while preserving human agency?

## A Brief Aside: Your Mind Already Extends Beyond Your Skull

Clark and Chalmers (1998) made a simple but radical argument: if an external process functions identically to an internal cognitive process, location doesn't matter. 

Their thought experiment: Otto has Alzheimer's and writes everything in his notebook. Inga has normal memory. When both navigate to the museum, Otto uses his notebook, Inga uses her biological memory. Same cognitive function, different substrate. The notebook *is* Otto's memory.

Otto's notebook can't think. Yet philosophers accept it as cognitive extension if it meets three criteria: constant accessibility, automatic endorsement, and easy retrieval. Your smartphone already qualifies. But AI with memory obliterates these criteria—it's Otto's notebook that can reason, adapt, and respond.

So the question isn't whether AI can become part of our extended cognition. The question is: what KIND of cognitive extension should it be?

## The Proposal: Supporting Characters as Cognitive Prosthetics

Interacting with actual characters feels fundamentally different than prompting a generic assistant. 

ChatGPT explains productivity systems. McGonagall simply raises an eyebrow at your excuses, then helps you find the discipline you had all along. ChatGPT offers coping strategies. Dr. Watson provides the quiet companionship of someone who's walked difficult paths, sitting with you through hard times without pretending to cure them.

This difference matters because character-driven AI offers a solution to the dependency trap. Heinz von Foerster, the Austrian-American cyberneticist, gave us the design principles we need. His imperatives—developed for human systems—turn out to be perfect guardrails for AI:

- **Ethical imperative**: "Act always so as to increase the number of choices"
- **Aesthetic imperative**: "If you desire to see, learn how to act"

Supporting characters—whether from literature, popular culture, or modeled on real experts—naturally embody these principles. They can't solve your problems directly; they can only work with you to find solutions. They expand choices through questions, teach through interaction, and maintain boundaries that preserve agency. McGonagall won't do your homework—she'll help you plan your study schedule. Watson won't diagnose or cure your condition—he'll provide companionship while you process it.

Unlike generic AI that aims to please, characters have authentic boundaries. When McGonagall refuses to write your essay, that's not system failure—that's McGonagall being McGonagall. The constraint becomes a feature.

And here's how we know it's working: users consistently report these interactions as 'fun' or 'engaging.' The enjoyment is the subjective experience of successful cognitive coupling. When McGonagall feels natural to banter with, that feeling IS your brain recognizing a compatible cognitive extension. Fun isn't separate from functionality; it's diagnostic data that the interface matches your cognitive architecture.

## Another Brief Aside: Why Your Brain Loves This

Why do character interfaces work so well? Because your brain runs character simulations all day long.

Your Default Mode Network—active when "doing nothing"—writes fanfiction about your life. Buckner et al. (2008) showed the same brain regions activate when remembering the past and imagining the future. We have two fundamental thinking modes: paradigmatic (logic, universal truth) and narrative (meaning through stories).

The evidence is striking: patients with hippocampal damage don't just lose memories—they lose narrative continuity, existing in "permanent present tense." Without stories, there's no continuous self.

So character-based AI isn't frivolous—it's cognitively ergonomic. These interfaces work because they match the narrative architecture they're extending.

## The Mechanics: How Supporting Characters Actually Work

### Core Principles

Literature figured out supporting characters centuries ago. The best ones:

- **Ask questions instead of giving orders**: McGonagall asking "What's truly preventing you from starting?" beats generic productivity advice
- **Complement rather than replace abilities**: Watson's companionship enhances your processing without replacing your insights
- **Provide temporary scaffolding then step back**: Like Sam with the ring—taking burdens only when necessary, returning them immediately
- **Maintain explicit boundaries**: McGonagall won't do your homework; that limitation fosters growth

### Character-Domain Matching

You can't slap any personality onto any domain. McGonagall works for time management because she embodies exactly what's needed: discipline with compassion, high standards with understanding. When she tells you to prioritize properly, it carries weight because that's what McGonagall *would* say.

But imagine McGonagall teaching negotiation. "Mr. Potter, one simply does not haggle!" It doesn't work. Negotiation requires flexibility, strategic thinking, and comfort with moral ambiguity—Tyrion territory.

The best matches feel inevitable in hindsight:
- **McGonagall + Time Management**: Academic discipline meets caring strictness
- **Watson + Companionship**: Loyal presence with emotional wisdom, not clinical intervention

The character framework extends beyond fictional personalities. Some practitioners create agents based on real authors and experts, using their published works to capture both ideas and voice. A consulting agent might embody a business theorist's principles while matching their communication style. These follow the same design patterns—consistent personality, domain expertise, natural boundaries—just drawn from different source material.

### The Human Prompting Toolkit

Just as AI has Chain of Thought prompting, character-driven agents need communication methods optimized for human neural networks:

**The Socratic Method**: Guide discovery through questions. When McGonagall asks "What's truly preventing you from starting?" she's activating your analytical machinery.

**Dennett's Criticism Protocol**: Build trust before challenging ideas—restate positions well, find agreement, acknowledge learning, then offer alternatives.

**Tactical Empathy**: Create psychological safety before introducing challenging ideas. When Watson says "That sounds like a particularly difficult day," he's creating space for you to process.

**Narrative Anchoring**: Instead of "Prioritize important tasks," McGonagall might say "Remember when Hermione tried to take every single class with the Time-Turner?" Same principle, 10x more memorable.

### The Model Prompting Toolkit

Creating character-driven AI requires specific design techniques that preserve human agency while building trust:

**Von Foerster Heuristics**: Design prompts that expand rather than narrow choices. Characters should open possibilities ("What options are you considering?") not close them ("I'll handle that for you").

**Boundary Specification**: Explicitly define what the model won't do. Clear refusal triggers prevent the model from enabling dysfunction. McGonagall won't write essays, Watson won't provide diagnoses—only companionship.

**Refusal Design**: Make boundaries feel like personality, not system limitation. When McGonagall says "That's something you must do yourself, Potter," it feels caring. When generic AI says "I can't do that," it feels broken.

**Accuracy Techniques**: Build trust through reliability. Include explicit instructions: "Never hallucinate facts or fabricate sources. Admit uncertainty rather than confabulate." Repeat these requirements strategically throughout the prompt.

## The Bottom Line

Character-driven AI works because it matches how humans think and learn. These systems function as "narrato-cognitive prosthetics" (yes, that term is ridiculous—but so is the fact that talking to fictional characters actually helps people) by hooking into narrative processing, creating memorable contexts, and preserving agency through character dynamics.

People are already using AI primarily for companionship and support. The question is how to design these systems to enhance rather than diminish capability. McGonagall helps you find your own discipline, not dependency. Watson provides presence, not solutions.

We're already building AI that works with our narrative brains instead of against them. The future isn't making AI smarter—it's making AI that makes us smarter, one well-crafted conversation at a time.

Speaking of well-crafted conversations: the characters discussed here—McGonagall, Watson, and others—were built using PEA, a prompt engineering assistant who embodies these very principles. (Yes, we gave the essay about character-driven AI to an AI character who helps build AI characters. The loops are getting strange.) You can explore and download PEA and these implementations on [Github](https://github.com/dmbch/pea).